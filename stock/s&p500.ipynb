{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "import datetime as dt\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as  web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "style.use('ggplot')\n",
    "def visualize_data():\n",
    "    df=pd.read_csv('sp500_joined_closes.csv')\n",
    "    \n",
    "    # cont corrlextion between database of file.\n",
    "    df_corr=df.corr()   \n",
    "    print(df_corr.head())\n",
    "    \n",
    "    #Drow plot from df_corr \n",
    "    \n",
    "    data=df.corr.values\n",
    "    fig=plt.figure()\n",
    "    ax= fig.add_subplot(111)\n",
    "    \n",
    "    heatmap= ax.pcolor(data, cmap=plt.cm.RDLGn )\n",
    "    fig.colorbar(heatmap)\n",
    "    ax.set_xticks(np.arange(data.shape[0])+0.5, minor=False)\n",
    "    ax.set_yticks(np.arange(data.shape[1])+0.5, minor=False)\n",
    "    \n",
    "    ax.invert_yaxis( )\n",
    "    ax.xaxs.tick_top()\n",
    "    \n",
    "    column_labels=df_corr.columns\n",
    "    row_lables=df.corr.ndex\n",
    "    \n",
    "    ax.set_xticklabels(column_labels)\n",
    "    ax.set_yticklabels(row_labels)\n",
    "    \n",
    "    plt.xtickes(rotation=90)\n",
    "    \n",
    "    heatmap.set_clime(-1,1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'sp500_joined_closes.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-dce67476d1b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mvisualize_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-96e5129f6700>\u001b[0m in \u001b[0;36mvisualize_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvisualize_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sp500_joined_closes.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# cont corrlextion between database of file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdf_corr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'sp500_joined_closes.csv' does not exist"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    visualize_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print('Downloading data from Yahoo for %s sector' % sector)? (<ipython-input-7-fbecfa4d111e>, line 37)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-fbecfa4d111e>\"\u001b[0;36m, line \u001b[0;32m37\u001b[0m\n\u001b[0;31m    print 'Downloading data from Yahoo for %s sector' % sector\u001b[0m\n\u001b[0m                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print('Downloading data from Yahoo for %s sector' % sector)?\n"
     ]
    }
   ],
   "source": [
    "import urllib2\n",
    "import pytz\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from pandas.io.data import DataReader\n",
    "\n",
    "\n",
    "SITE = \"http://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "START = datetime(1900, 1, 1, 0, 0, 0, 0, pytz.utc)\n",
    "END = datetime.today().utcnow()\n",
    "\n",
    "\n",
    "def scrape_list(site):\n",
    "    hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "    req = urllib2.Request(site, headers=hdr)\n",
    "    page = urllib2.urlopen(req)\n",
    "    soup = BeautifulSoup(page)\n",
    "\n",
    "    table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "    sector_tickers = dict()\n",
    "    for row in table.findAll('tr'):\n",
    "        col = row.findAll('td')\n",
    "        if len(col) > 0:\n",
    "            sector = str(col[3].string.strip()).lower().replace(' ', '_')\n",
    "            ticker = str(col[0].string.strip())\n",
    "            if sector not in sector_tickers:\n",
    "                sector_tickers[sector] = list()\n",
    "            sector_tickers[sector].append(ticker)\n",
    "    return sector_tickers\n",
    "\n",
    "\n",
    "def download_ohlc(sector_tickers, start, end):\n",
    "    sector_ohlc = {}\n",
    "    for sector, tickers in sector_tickers.iteritems():\n",
    "        print 'Downloading data from Yahoo for %s sector' % sector\n",
    "        data = DataReader(tickers, 'yahoo', start, end)\n",
    "        for item in ['Open', 'High', 'Low']:\n",
    "            data[item] = data[item] * data['Adj Close'] / data['Close']\n",
    "        data.rename(items={'Open': 'open', 'High': 'high', 'Low': 'low',\n",
    "                           'Adj Close': 'close', 'Volume': 'volume'},\n",
    "                    inplace=True)\n",
    "        data.drop(['Close'], inplace=True)\n",
    "        sector_ohlc[sector] = data\n",
    "    print 'Finished downloading data'\n",
    "    return sector_ohlc\n",
    "\n",
    "\n",
    "def store_HDF5(sector_ohlc, path):\n",
    "    with pd.get_store(path) as store:\n",
    "        for sector, ohlc in sector_ohlc.iteritems():\n",
    "            store[sector] = ohlc\n",
    "\n",
    "\n",
    "def get_snp500():\n",
    "    sector_tickers = scrape_list(SITE)\n",
    "    sector_ohlc = download_ohlc(sector_tickers, START, END)\n",
    "    store_HDF5(sector_ohlc, 'snp500.h5')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    get_snp500()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module googlefinance.client in googlefinance:\n",
      "\n",
      "NAME\n",
      "    googlefinance.client - # coding: utf-8\n",
      "\n",
      "FUNCTIONS\n",
      "    get_closing_data(queries, period)\n",
      "    \n",
      "    get_open_close_data(queries, period)\n",
      "    \n",
      "    get_price_data(query)\n",
      "    \n",
      "    get_prices_data(queries, period)\n",
      "    \n",
      "    get_prices_time_data(queries, period, interval)\n",
      "\n",
      "FILE\n",
      "    /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/googlefinance/client.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import googlefinance.client\n",
    "help(googlefinance.client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GET_DATA_FROM_GOOGLE(reload_sp500=False ):\n",
    "    if reload_sp500:\n",
    "         tickers=SAVE_SP500_TICKERS()\n",
    "    else:\n",
    "        with open('sp500tickers.pickle','rb') as f:\n",
    "            tickers=pickle.load(f)\n",
    "    if not os.path.exists('stock_dfs '):\n",
    "        os.makedirs('stock_dfs') \n",
    "        \n",
    "        \n",
    "    start=dt.datetime(2012,1,1)\n",
    "    #end=dt.datetime(2017,1,1)\n",
    "    end   = dt.date.today()\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        if not os.path.exists('stock_dfs/{}.csv'.format(ticker) ):\n",
    "            df=web.DataReader(ticker,'morningstar',start,end)\n",
    "            #param = {\n",
    "                #'q': \"/{}\".format(ticker) # Stock symbol (ex: \"AAPL\")\n",
    "                #'i': \"86400\", # Interval size in seconds (\"86400\" = 1 day intervals)\n",
    "                #'x': \"NASD\", # Stock exchange symbol on which stock is traded (ex: \"NASD\")\n",
    "                #'p': \"1Y\" # Period (Ex: \"1Y\" = 1 year)\n",
    "            #}\n",
    "            #df = get_price_data(param)\n",
    "            df.to_csv('stock_dfs/{}.csv'.format(ticker))\n",
    "        else :\n",
    "            print('ALready have {}'.format(tikcer ))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "import datetime as dt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from googlefinance.client import get_price_data, get_prices_data, get_prices_time_data\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as  web\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import quandl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505\n"
     ]
    }
   ],
   "source": [
    "def SAVE_SP500_TICKERS():\n",
    "    resp=requests.get('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "    soup=bs.BeautifulSoup(resp.text,'lxml')\n",
    "    table=soup.find('table',{'class':\"wikitable sortable\"})\n",
    "    tickers=[]\n",
    "    for row in table.findAll('tr')[1:]:\n",
    "        ticker=row.findAll('td')[0].text\n",
    "        tickers.append(ticker)\n",
    "        \n",
    "    with open('sp500tickers.pickle','wb') as f:\n",
    "        pickle.dump(tickers,f)\n",
    "        \n",
    "    #print(tickers)\n",
    "    \n",
    "    return tickers\n",
    "\n",
    "\n",
    "sa=SAVE_SP500_TICKERS()\n",
    "print( len(sa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding ANDV to retry list\n"
     ]
    }
   ],
   "source": [
    "import bs4 as bs\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "import bs4 as bs\n",
    "import datetime as dt\n",
    "import os\n",
    "import pandas_datareader.data as web\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "def save_sp500_tickers():\n",
    "    resp = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "    soup = bs.BeautifulSoup(resp.text, 'lxml')\n",
    "    table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "    tickers = []\n",
    "    for row in table.findAll('tr')[1:]:\n",
    "        ticker = row.findAll('td')[0].text\n",
    "        tickers.append(ticker)\n",
    "\n",
    "    with open(\"sp500tickers.pickle\",\"wb\") as f:\n",
    "        pickle.dump(tickers,f)\n",
    "        \n",
    "\n",
    "# save_sp500_tickers()\n",
    "def get_data_from_yahoo(reload_sp500=False):\n",
    "    if reload_sp500:\n",
    "        tickers = save_sp500_tickers()\n",
    "    else:\n",
    "        with open(\"sp500tickers.pickle\", \"rb\") as f:\n",
    "            tickers = pickle.load(f)\n",
    "    if not os.path.exists('stock_dfs'):\n",
    "        os.makedirs('stock_dfs')\n",
    "\n",
    "    start = dt.datetime(2010, 1, 1)\n",
    "    end = dt.datetime.now()\n",
    "    for ticker in tickers:\n",
    "        # just in case your connection breaks, we'd like to save our progress!\n",
    "        if not os.path.exists('stock_dfs/{}.csv'.format(ticker)):\n",
    "            df = web.DataReader(ticker, 'morningstar', start, end)\n",
    "            df.reset_index(inplace=True)\n",
    "            df.set_index(\"Date\", inplace=True)\n",
    "            df = df.drop(\"Symbol\", axis=1)\n",
    "            df.to_csv('stock_dfs/{}.csv'.format(ticker))\n",
    "        else:\n",
    "            print('Already have {}'.format(ticker))\n",
    "\n",
    "\n",
    "get_data_from_yahoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already have FLIR\n",
      "Already have FLS\n",
      "Already have FLR\n",
      "Already have FMC\n",
      "Already have FL\n",
      "Already have F\n",
      "Already have FTV\n",
      "Already have FBHS\n",
      "Already have BEN\n",
      "Already have FCX\n",
      "Already have GPS\n",
      "Already have GRMN\n",
      "Already have IT\n",
      "Already have GD\n",
      "Already have GE\n",
      "Already have GGP\n",
      "Already have GIS\n",
      "Already have GM\n",
      "Already have GPC\n",
      "Already have GILD\n",
      "Already have GPN\n",
      "Already have GS\n",
      "Already have GT\n",
      "Already have GWW\n",
      "Already have HAL\n",
      "Already have HBI\n",
      "Already have HOG\n",
      "Already have HRS\n",
      "Already have HIG\n",
      "Already have HAS\n",
      "Already have HCA\n",
      "Already have HCP\n",
      "Already have HP\n",
      "Already have HSIC\n",
      "Already have HSY\n",
      "Already have HES\n",
      "Already have HPE\n",
      "Already have HLT\n",
      "Already have HFC\n",
      "Already have HOLX\n",
      "Already have HD\n",
      "Already have HON\n",
      "Already have HRL\n",
      "Already have HST\n",
      "Already have HPQ\n",
      "Already have HUM\n",
      "Already have HBAN\n",
      "Already have HII\n",
      "Already have IDXX\n",
      "Already have INFO\n",
      "Already have ITW\n",
      "Already have ILMN\n",
      "Already have IR\n",
      "Already have INTC\n",
      "Already have ICE\n",
      "Already have IBM\n",
      "Already have INCY\n",
      "Already have IP\n",
      "Already have IPG\n",
      "Already have IFF\n",
      "Already have INTU\n",
      "Already have ISRG\n",
      "Already have IVZ\n",
      "Already have IPGP\n",
      "Already have IQV\n",
      "Already have IRM\n",
      "Already have JEC\n",
      "Already have JBHT\n",
      "Already have JEF\n",
      "Already have SJM\n",
      "Already have JNJ\n",
      "Already have JCI\n",
      "Already have JPM\n",
      "Already have JNPR\n",
      "Already have KSU\n",
      "Already have K\n",
      "Already have KEY\n",
      "Already have KMB\n",
      "Already have KIM\n",
      "Already have KMI\n",
      "Already have KLAC\n",
      "Already have KSS\n",
      "Already have KHC\n",
      "Already have KR\n",
      "Already have LB\n",
      "Already have LLL\n",
      "Already have LH\n",
      "Already have LRCX\n",
      "Already have LEG\n",
      "Already have LEN\n",
      "Already have LLY\n",
      "Already have LNC\n",
      "Already have LKQ\n",
      "Already have LMT\n",
      "Already have L\n",
      "Already have LOW\n",
      "Already have LYB\n",
      "Already have MTB\n",
      "Already have MAC\n",
      "Already have M\n",
      "Already have MRO\n",
      "Already have MPC\n",
      "Already have MAR\n",
      "Already have MMC\n",
      "Already have MLM\n",
      "Already have MAS\n",
      "Already have MA\n",
      "Already have MAT\n",
      "Already have MKC\n",
      "Already have MCD\n",
      "Already have MCK\n",
      "Already have MDT\n",
      "Already have MRK\n",
      "Already have MET\n",
      "Already have MTD\n",
      "Already have MGM\n",
      "Already have KORS\n",
      "Already have MCHP\n",
      "Already have MU\n",
      "Already have MSFT\n",
      "Already have MAA\n",
      "Already have MHK\n",
      "Already have TAP\n",
      "Already have MDLZ\n",
      "Already have MNST\n",
      "Already have MCO\n",
      "Already have MS\n",
      "Already have MOS\n",
      "Already have MSI\n",
      "Already have MSCI\n",
      "Already have MYL\n",
      "Already have NDAQ\n",
      "Already have NOV\n",
      "Already have NKTR\n",
      "Already have NTAP\n",
      "Already have NFLX\n",
      "Already have NWL\n",
      "Already have NFX\n",
      "Already have NEM\n",
      "Already have NWSA\n",
      "Already have NWS\n",
      "Already have NEE\n",
      "Already have NLSN\n",
      "Already have NKE\n",
      "Already have NI\n",
      "Already have NBL\n",
      "Already have JWN\n",
      "Already have NSC\n",
      "Already have NTRS\n",
      "Already have NOC\n",
      "Already have NCLH\n",
      "Already have NRG\n",
      "Already have NUE\n",
      "Already have NVDA\n",
      "Already have ORLY\n",
      "Already have OXY\n",
      "Already have OMC\n",
      "Already have OKE\n",
      "Already have ORCL\n",
      "Already have PCAR\n",
      "Already have PKG\n",
      "Already have PH\n",
      "Already have PAYX\n",
      "Already have PYPL\n",
      "Already have PNR\n",
      "Already have PBCT\n",
      "Already have PEP\n",
      "Already have PKI\n",
      "Already have PRGO\n",
      "Already have PFE\n",
      "Already have PCG\n",
      "Already have PM\n",
      "Already have PSX\n",
      "Already have PNW\n",
      "Already have PXD\n",
      "Already have PNC\n",
      "Already have RL\n",
      "Already have PPG\n",
      "Already have PPL\n",
      "Already have PX\n",
      "Already have PFG\n",
      "Already have PG\n",
      "Already have PGR\n",
      "Already have PLD\n",
      "Already have PRU\n",
      "Already have PEG\n",
      "Already have PSA\n",
      "Already have PHM\n",
      "Already have PVH\n",
      "Already have QRVO\n",
      "Already have PWR\n",
      "Already have QCOM\n",
      "Already have DGX\n",
      "Already have RJF\n",
      "Already have RTN\n",
      "Already have O\n",
      "Already have RHT\n",
      "Already have REG\n",
      "Already have REGN\n",
      "Already have RF\n",
      "Already have RSG\n",
      "Already have RMD\n",
      "Already have RHI\n",
      "Already have ROK\n",
      "Already have COL\n",
      "Already have ROP\n",
      "Already have ROST\n",
      "Already have RCL\n",
      "Already have CRM\n",
      "Already have SBAC\n",
      "Already have SCG\n",
      "Already have SLB\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n"
     ]
    }
   ],
   "source": [
    "import bs4 as bs\n",
    "import datetime as dt\n",
    "import os\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "import pickle\n",
    "import requests\n",
    "import quandl\n",
    "\n",
    "import time\n",
    "import fix_yahoo_finance as yf  \n",
    "from iexfinance import get_historical_data\n",
    "\n",
    "def save_sp500_tickers():\n",
    "    resp = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "    soup = bs.BeautifulSoup(resp.text, 'lxml')\n",
    "    table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "    tickers = []\n",
    "    for row in table.findAll('tr')[1:]:\n",
    "        ticker = row.findAll('td')[0].text\n",
    "        tickers.append(ticker)\n",
    "    with open(\"sp500tickers.pickle\", \"wb\") as f:\n",
    "        pickle.dump(tickers, f)\n",
    "    return tickers\n",
    "\n",
    "\n",
    "# save_sp500_tickers()\n",
    "def get_data_from_yahoo(reload_sp500=False):\n",
    "    if reload_sp500:\n",
    "        tickers = save_sp500_tickers()\n",
    "    else:\n",
    "        with open(\"sp500tickers.pickle\", \"rb\") as f:\n",
    "            tickers = pickle.load(f)\n",
    "    if not os.path.exists('stock_dfs'):\n",
    "        os.makedirs('stock_dfs')\n",
    "\n",
    "    start = dt.datetime(2015, 1, 1)\n",
    "    end = dt.datetime.now()\n",
    "    for ticker in range(200,len(tickers)):\n",
    "        # just in case your connection breaks, we'd like to save our progress!\n",
    "        if not os.path.exists('stock_dfs/{}.csv'.format(tickers[ticker])):\n",
    "            #df = web.DataReader(tickers[ticker], 'morningstar', start, end)\n",
    "            \n",
    "            #df = quandl.get(\"EOD/{}\".format(tickers[ticker]), authtoken=\"KxKziu7vxQ91gSPCxGnj\")\n",
    "            #df = yf.download(tickers[ticker],start,end)\n",
    "            #df = quandl.get(\"WIKI/{}\".format(tickers[ticker]),start_date=start,end_date=end)\n",
    "            \n",
    "            df = get_historical_data(tickers[ticker], start=start, end=end, output_format='pandas')\n",
    "            df.reset_index(inplace=True)\n",
    "            #df.set_index(\"Date\", inplace=True)\n",
    "            #df = df.drop(\"Symbol\", axis=1)\n",
    "            df.to_csv('stock_dfs/{}.csv'.format(tickers[ticker]))\n",
    "            time.sleep(30)\n",
    "            print(ticker)\n",
    "        else:\n",
    "            print('Already have {}'.format(tickers[ticker]))\n",
    "\n",
    "\n",
    "            \n",
    "get_data_from_yahoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://www.quora.com/Using-Python-whats-the-best-way-to-get-stock-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
